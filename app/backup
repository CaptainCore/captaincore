#!/bin/bash

#
#   Backups one or more sites.
#
#   `captaincore backup`
#
#   [<site>...]
#   One or more sites to backup.
#
#   [--all]
#   Backup all sites.
#
#   [--use-direct]
#   (Pull and Push) Directly from sftp to Rclone remote
#
#   [--skip-remote]
#   (Pull Only) Skips push to Rclone remote (define $rclone_backup in config)
#
#   [--skip-db]
#   Skips database backup
#
#   [--with-staging]
#   Backups staging site
#

# Load configuration
root_path="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"; root_path=${root_path%app*}
source ${root_path}config
source ${root_path}/lib/arguments

run_command () {
  if [ $# -gt 0 ]; then

    ### Generate random auth
    auth=''; for count in {0..6}; do auth+=$(printf "%x" $(($RANDOM%16)) ); done;

    ### Begin time tracking
    overalltimebegin=$(date +"%s")
    backup_date=$(date +'%Y-%m-%d')
    backup_time=$(date +'%H-%M')

    ### Define log file format
    logs_path=$logs/$backup_date/$backup_time-$auth

    ### Generate log folder
    mkdir -p $logs_path

    ### Begin logging
    echo "$(date +'%Y-%m-%d %H:%M') Begin server backup" > $logs_path/backup-log.txt

    echo "Backing up $# sites"
    INDEX=1
    for site in "$@"; do

      ### Load FTP credentials
      eval $(captaincore site get $site --bash)

      ### If subsite update stats and skip backup
      if [[ $subsite == "true" ]]; then

        ### Views for yearly stats
        views=`captaincore stats $domain`

        ### Updates stats with no storage since it's a subsite
        curl --data-urlencode "storage=0" --data-urlencode "views=$views" --data-urlencode "token=$token" "$captaincore_api/$domain/"

      fi

      ### Credentials found, start the backup
      if ! [ -z "$domain" ]; then

        if [ "$homedir" == "" ]; then
          homedir="/"
        fi

        ### Lookup rclone
        remotes=$(rclone listremotes)

        ### Check for rclone remote
        rclone_remote_lookup=false
        for item in ${remotes[@]}; do
          if [[ sftp-$site: == "$item" ]]; then
            rclone_remote_lookup=true
          fi
        done

        if [[ $rclone_remote_lookup == false ]]; then
          echo "$(date +'%Y-%m-%d %H:%M') Generating rclone configs for $site" >> $logs_path/backup-log.txt
          echo "$(date +'%Y-%m-%d %H:%M') Generating rclone configs for $site"
          captaincore site rclone-configs $site
        fi

        # captures FTP errors in $ftp_output and file listing to log file
        ftp_output=$( { rclone lsd sftp-$site:$homedir ; } 2>&1 )
        ftp_search_for_wordpress=`echo "$ftp_output" | perl -wnE'say for /wp-admin/g'`

        # Handle FTP errors
        if [[ $ftp_search_for_wordpress != "wp-admin" ]]; then

          # WordPress not found, so performing regular backup

          # Backup site locally
          rclone sync sftp-$site:$homedir $path/$site/backup/ --exclude .DS_Store --exclude *timthumb.txt --exclude /wp-content/uploads_from_s3/ --verbose=1 --log-file="$logs_path/site-$site.txt"
          echo "" >> $logs_path/site-$site.txt
          tail $logs_path/site-$site.txt >> $logs_path/backup-local.txt

          if [[ "$OSTYPE" == "linux-gnu" ]]; then
            ### Begin folder size in bytes without apparent-size flag
            folder_size=`du -s --block-size=1 $path/$site/backup/`
            folder_size=`echo $folder_size | cut -d' ' -f 1`
          elif [[ "$OSTYPE" == "darwin"* ]]; then
            ### Calculate folder size in bytes http://superuser.com/questions/22460/how-do-i-get-the-size-of-a-linux-or-mac-os-x-directory-from-the-command-line
            folder_size=`find $path/$site/backup/ -type f -print0 | xargs -0 stat -f%z | awk '{b+=$1} END {print b}'`
          fi

          if [[ $skip_remote != true ]]; then

            ### Incremental backup upload to Remote
            echo "$(date +'%Y-%m-%d %H:%M') Queuing incremental backup $site to remote (${INDEX}/$#)" >> $logs_path/backup-log.txt
            echo "$(date +'%Y-%m-%d %H:%M') Queuing incremental backup $site to remote (${INDEX}/$#)"
            ts rclone sync $path/$site/backup $rclone_backup/$site -v --exclude .DS_Store --fast-list --transfers=32 --log-file="$logs_path/site-$site-remote.txt"

            ### Add site to Remote log file
            ### Grabs last 6 lines of output from remote transfer to log file
            ts sh -c "echo \"Finished remote backup $site (${INDEX}/$#)\" >> $logs_path/backup-remote.txt && tail -6 $logs_path/site-$site-remote.txt >> $logs_path/backup-remote.txt"

            # Post folder size bytes and yearly views to ACF field
            curl --data-urlencode "storage=$folder_size" --data-urlencode "token=$token" "$captaincore_api/$domain/"
          fi

          ## Add FTP error to log file
          # echo "WordPress not found, performing regular backup: $site ($ftp_output)<br>" >> $logs_path/backup-log.txt
          echo "WordPress not found, performing regular backup: $site ($ftp_output)"
        else
          ## No errors found, run the backup

          ### Incremental backup locally with rclone

          echo "$(date +'%Y-%m-%d %H:%M') Begin incremental backup $site to local (${INDEX}/$#)" >> $logs_path/backup-log.txt
          echo "$(date +'%Y-%m-%d %H:%M') Begin incremental backup $site to local (${INDEX}/$#)"

          if [[ $skip_db != true ]]; then
            ## Database backup (if remote server available)
            if [[ "$address" == *".kinsta.com" ]] || [[ "$address" == *".wpengine.com" ]]; then
              captaincore ssh $site --command="wp db export --skip-plugins --skip-themes --add-drop-table - > wp-content/mysql.sql"
            fi
          fi

          # Backup site locally
          rclone sync sftp-$site:$homedir $path/$site/backup/ --exclude .DS_Store --exclude *timthumb.txt --exclude /wp-content/uploads_from_s3/ --verbose=1 --log-file="$logs_path/site-$site.txt"
          echo "" >> $logs_path/site-$site.txt
          tail $logs_path/site-$site.txt >> $logs_path/backup-local.txt

          ## Backup S3 uploads if needed
          if [ -n "$s3bucket" ]; then
            echo "$(date +'%Y-%m-%d %H:%M') Begin incremental backup $site (S3) to local (${INDEX}/$#)" >> $logs_path/backup-log.txt
            echo "$(date +'%Y-%m-%d %H:%M') Begin incremental backup $site (S3) to local (${INDEX}/$#)"
            rclone sync s3-$site:$s3bucket/$s3path $path/$site/backup/wp-content/uploads_from_s3/ --exclude .DS_Store --exclude *timthumb.txt --verbose=1 --log-file="$logs_path/site-$site-s3.txt"
          fi

          if [[ "$OSTYPE" == "linux-gnu" ]]; then
            ### Begin folder size in bytes without apparent-size flag
            folder_size=`du -s --block-size=1 $path/$site/backup/`
            folder_size=`echo $folder_size | cut -d' ' -f 1`
          elif [[ "$OSTYPE" == "darwin"* ]]; then
            ### Calculate folder size in bytes http://superuser.com/questions/22460/how-do-i-get-the-size-of-a-linux-or-mac-os-x-directory-from-the-command-line
            folder_size=`find $path/$site/backup/ -type f -print0 | xargs -0 stat -f%z | awk '{b+=$1} END {print b}'`
          fi

          if [[ $skip_remote != true ]]; then

            ### Incremental backup upload to Remote
            echo "$(date +'%Y-%m-%d %H:%M') Queuing incremental backup $site to remote (${INDEX}/$#)" >> $logs_path/backup-log.txt
            echo "$(date +'%Y-%m-%d %H:%M') Queuing incremental backup $site to remote (${INDEX}/$#)"
            ts rclone sync $path/$site/backup/ $rclone_backup/$site -v --exclude .DS_Store --fast-list --transfers=32 --log-file="$logs_path/site-$site-remote.txt"

            ### Add site to Remote log file
            ### Grabs last 6 lines of output from remote transfer to log file
            ts sh -c "echo \"Finished remote backup $site (${INDEX}/$#)\" >> $logs_path/backup-remote.txt && tail -6 $logs_path/site-$site-remote.txt >> $logs_path/backup-remote.txt"

            ### Views for yearly stats
            views=`captaincore stats $domain`

            # Post folder size bytes and yearly views to ACF field
            curl --data-urlencode "storage=$folder_size" --data-urlencode "views=$views" --data-urlencode "token=$token" "$captaincore_api/$domain/"
          fi

        fi

      fi

      ### Clear out variables
      domain=''
      username=''
      password=''
      address=''
      protocol=''
      port=''
      homedir=''
      remoteserver=''
      s3bucket=''
      s3path=''
      subsite=''

      let INDEX=${INDEX}+1
    done

    echo "$(date +'%Y-%m-%d %H:%M') Finishing queued remote backups"
    echo "$(date +'%Y-%m-%d %H:%M') Finishing queued remote backups" >> $logs_path/backup-log.txt
    ts -w
    echo "$(date +'%Y-%m-%d %H:%M') Finished queued remote backups"
    echo "$(date +'%Y-%m-%d %H:%M') Finished queued remote backups" >> $logs_path/backup-log.txt

    ### End time tracking
    overalltimeend=$(date +"%s")
    diff=$(($overalltimeend-$overalltimebegin))
    echo "$(date +'%Y-%m-%d %H:%M') $(($diff / 3600)) hours, $((($diff / 60) % 60)) minutes and $(($diff % 60)) seconds elapsed." >> $logs_path/backup-log.txt

    ### Generate logs
    cd $logs_path
    tar -cvzf logs.tar.gz *

    ### Upload logs to Dropbox
    rclone sync $logs/$backup_date/$backup_time-$auth Anchor-Dropbox:Backup/Logs/$backup_date/$backup_time-$auth --exclude .DS_Store

    ### Generate dropbox link to logs
    shareurl=`captaincore utils dropbox_uploader share Backup/Logs/$backup_date/$backup_time-$auth`
    shareurl=`echo $shareurl | grep -o 'https.*'`

    ### Generate overall emails
    ( echo "$(captaincore get transferred_stats file=$logs_path/backup-remote.txt)" && printf "<br><a href='$shareurl'>View Logs</a><br><br>" && grep -r "FTP response" $logs_path/backup-log.txt; ) \
      | mutt -e 'set content_type=text/html' -s "Backup completed: $# sites | $backup_date" -a $logs_path/backup-log.txt -- support@anchor.host

    cd $path

  fi
}

# See if any sites are specifed
if [ ${#arguments[*]} -gt 0 ]; then
  # Runs on specifed sites
  run_command ${arguments[*]}
fi

if [[ $all == "true" ]]; then
  # Runs on all sites
	run_command ${websites[@]}
fi

# Error if no sites specifed
if [[ $all != "true" ]] && [ ${#arguments[*]} -eq 0 ]; then
  echo -e "${COLOR_RED}Error:${COLOR_NORMAL} Please specify one or more sites, or use --all."
fi
