#!/usr/bin/env bash

#
#   Backups one or more sites.
#
#   `captaincore backup`
#
#   [<site>...]
#   One or more sites to backup.
#
#   [--all]
#   Backup all sites.
#
#   [--use-direct]
#   (Pull and Push) Directly from sftp to Rclone remote
#
#   [--skip-remote]
#   (Pull Only) Skips push to Rclone remote (define $rclone_backup in config)
#
#   [--skip-db]
#   Skips database backup
#
#   [--email-notify]
#   Sends email summary on completion
#

# Load configuration
root_path="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"; root_path=${root_path%app*}
source ${root_path}lib/arguments

run_command() {

  # Generate random auth
  auth=''; for count in {0..6}; do auth+=$(printf "%x" $(($RANDOM%16)) ); done;

  # Begin time tracking
  overalltimebegin=$(date +"%s")
  backup_date=$(date +'%Y-%m-%d')
  backup_time=$(date +'%H-%M')

  # Define log file format
  logs_path=$logs/$backup_date/$backup_time-$auth

  # Generate log folder
  mkdir -p $logs_path

  # Begin logging
  echo "$(date +'%Y-%m-%d %H:%M') Begin server backup" > $logs_path/backup-log.txt

  echo "Backing up $# sites"
  INDEX=1
  for site in "$@"; do

    # Extract environment
    if [[ "$site" == *"-staging"* ]]; then
      environment=staging
    else
      environment=production
    fi

    # Load site configs
    while read site_configs; do declare "$site_configs"; done <<< "$(captaincore site get $site --bash --captain_id=$captain_id)"

    # Site found, start the backup
    if [[ $domain == "" ]]; then
      echo "Error: $site missing domain."
      continue
    fi

    # Append trailing slash if home_directory exist
    if [ "$home_directory" != "" ]; then
      home_directory="${home_directory}/"
    fi

    # Lookup rclone
    remotes=$( rclone listremotes )

    # Check for rclone remote
    rclone_remote_lookup=false
    for item in ${remotes[@]}; do
      if [[ sftp-$site: == "$item" ]]; then
        rclone_remote_lookup=true
      fi
    done

    if [[ $rclone_remote_lookup == false ]]; then
      echo "$(date +'%Y-%m-%d %H:%M') Generating rclone configs for $site" >> $logs_path/backup-log.txt
      echo "$(date +'%Y-%m-%d %H:%M') Generating rclone configs for $site"
      captaincore rclone-configs $site
    fi

    # Captures FTP errors in $ftp_output and file listing to log file
    ftp_output=$( { rclone lsd sftp-$site:$home_directory ; } 2>&1 )
    ftp_search_for_wordpress=$( echo "$ftp_output" | perl -wnE'say for /wp-admin/g' )

    # Handle FTP errors
    if [[ $ftp_search_for_wordpress != "wp-admin" ]]; then
      wordpress_not_found=true
    fi

    # Incremental backup locally with rclone
    echo "$(date +'%Y-%m-%d %H:%M') Begin incremental backup $site to local (${INDEX}/$#)" >> $logs_path/backup-log.txt
    echo "$(date +'%Y-%m-%d %H:%M') Begin incremental backup $site to local (${INDEX}/$#)"

    if [[ $skip_db != true ]] && [[ $wordpress_not_found != true ]]; then

      # Database backup (if remote server available)
      if [[ "$provider" == "kinsta" ]] || [[ "$provider" == "wpengine" ]]; then
        captaincore ssh $site --command="wp db export --skip-plugins --skip-themes --add-drop-table - > wp-content/mysql.sql; chmod 600 wp-content/mysql.sql" --captain_id=$captain_id
      fi

    fi

    # Backup site locally
    rclone sync sftp-$site:$home_directory $path/${site}_${site_id}/${environment}/backup/ --exclude .DS_Store --exclude *timthumb.txt --exclude /wp-content/uploads_from_s3/ --verbose=1 --log-file="$logs_path/site-$site.txt"
    echo "" >> $logs_path/site-$site.txt
    tail $logs_path/site-$site.txt >> $logs_path/backup-local.txt

    # Backup S3 uploads if needed
    if [ -n "$s3bucket" ]; then

      echo "$(date +'%Y-%m-%d %H:%M') Begin incremental backup $site (S3) to local (${INDEX}/$#)" >> $logs_path/backup-log.txt
      echo "$(date +'%Y-%m-%d %H:%M') Begin incremental backup $site (S3) to local (${INDEX}/$#)"
      rclone sync s3-$site:$s3bucket/$s3path $path/${site}_${site_id}/${environment}/backup/wp-content/uploads_from_s3/ --exclude .DS_Store --exclude *timthumb.txt --verbose=1 --log-file="$logs_path/site-$site-s3.txt"

    fi

    if [[ "$OSTYPE" == "linux-gnu" ]]; then

      # Begin folder size in bytes without apparent-size flag
      folder_size=$( du -s --block-size=1 $path/${site}_${site_id}/${environment}/backup/ )
      folder_size=$( echo $folder_size | cut -d' ' -f 1 )

    fi

    if [[ "$OSTYPE" == "darwin"* ]]; then

      # Calculate folder size in bytes http://superuser.com/questions/22460/how-do-i-get-the-size-of-a-linux-or-mac-os-x-directory-from-the-command-line
      folder_size=$( find $path/${site}_${site_id}/${environment}/backup/ -type f -print0 | xargs -0 stat -f%z | awk '{b+=$1} END {print b}' )

    fi

    if [[ $skip_remote != true ]]; then

      # Incremental backup upload to Remote
      echo "$(date +'%Y-%m-%d %H:%M') Queuing incremental backup $site to remote (${INDEX}/$#)" >> $logs_path/backup-log.txt
      echo "$(date +'%Y-%m-%d %H:%M') Queuing incremental backup $site to remote (${INDEX}/$#)"
      ts rclone sync $path/${site}_${site_id}/${environment}/backup/ $rclone_backup/${site}_${site_id}/${environment}/backup/ -v --exclude .DS_Store --fast-list --transfers=16 --log-file="$logs_path/site-$site-remote.txt"

      # Add site to Remote log file
      # Grabs last 6 lines of output from remote transfer to log file
      ts sh -c "echo \"Finished remote backup $site (${INDEX}/$#)\" >> $logs_path/backup-remote.txt && tail -6 $logs_path/site-$site-remote.txt >> $logs_path/backup-remote.txt"

      # Views for yearly stats
      views=$( captaincore stats $site --captain_id=$captain_id )

      # Post folder size bytes and yearly views to ACF fields
      curl --request POST "$captaincore_api" --header "Content-Type: application/json" --data @- << EOF
{
"command":"usage-update",
"site_id":"$site_id",
"environment":"$environment",
"storage":"$folder_size",
"views":"$views",
"token":"$token"
}
EOF
    fi

    ### Clear out variables
    domain=''
    username=''
    password=''
    address=''
    protocol=''
    port=''
    home_directory=''
    remoteserver=''
    s3bucket=''
    s3path=''
    subsite=''
    wordpress_not_found=''

    let INDEX=${INDEX}+1
  done

  echo "$(date +'%Y-%m-%d %H:%M') Finishing queued remote backups"
  echo "$(date +'%Y-%m-%d %H:%M') Finishing queued remote backups" >> $logs_path/backup-log.txt
  ts -w
  echo "$(date +'%Y-%m-%d %H:%M') Finished queued remote backups"
  echo "$(date +'%Y-%m-%d %H:%M') Finished queued remote backups" >> $logs_path/backup-log.txt

  # End time tracking
  overalltimeend=$(date +"%s")
  diff=$(($overalltimeend-$overalltimebegin))
  echo "$(date +'%Y-%m-%d %H:%M') $(($diff / 3600)) hours, $((($diff / 60) % 60)) minutes and $(($diff % 60)) seconds elapsed." >> $logs_path/backup-log.txt

  # Generate logs
  cd $logs_path
  tar -cvzf logs.tar.gz *

  # Upload logs to Rclone
  rclone sync $logs/$backup_date/$backup_time-$auth $rclone_logs/$backup_date/$backup_time-$auth --exclude .DS_Store

  # Generate Rclone link to logs (https://rclone.org/commands/rclone_link/)
  shareurl=$(rclone link $rclone_logs/$backup_date/$backup_time-$auth)

  if [[ "$email_notify" == "true" ]]; then

    # Generate overall emails
    ( echo "$(captaincore get transferred-stats $logs_path/backup-remote.txt --captain_id=$captain_id)" && printf "<br><a href='$shareurl'>View Logs</a><br><br>" && grep -r "FTP response" $logs_path/backup-log.txt; ) \
      | mutt -e 'set content_type=text/html' -s "Backup completed: $# sites | $backup_date" -a $logs_path/backup-log.txt -- $captaincore_admin_email

  fi

  cd $path

}

# See if any sites are specifed
if [ ${#arguments[*]} -gt 0 ]; then
  # Runs on specifed sites
  run_command ${arguments[*]}
fi

if [[ $all == "true" ]]; then
  # Runs on all sites
  run_command ${websites[@]}
fi

# Error if no sites specifed
if [[ $all != "true" ]] && [ ${#arguments[*]} -eq 0 ]; then
  echo -e "${COLOR_RED}Error:${COLOR_NORMAL} Please specify one or more sites, or use --all."
fi
